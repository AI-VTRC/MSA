{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0343018",
   "metadata": {},
   "source": [
    "# Plant Pathology 2020 Keras Implementation\n",
    "- Dataset Analysis\n",
    "- CNN Model Optimization\n",
    "\n",
    "- Get Keras\n",
    "- Get Model drawing\n",
    "- Try Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "debcf278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.2\n",
      "  Downloading tensorflow-2.2.0-cp38-cp38-win_amd64.whl (459.2 MB)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in c:\\users\\nguye\\anaconda3\\lib\\site-packages (from tensorflow==2.2) (0.13.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\nguye\\anaconda3\\lib\\site-packages (from tensorflow==2.2) (1.1.0)\n",
      "Collecting tensorboard<2.3.0,>=2.2.0\n",
      "  Downloading tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\nguye\\anaconda3\\lib\\site-packages (from tensorflow==2.2) (1.12.1)\n",
      "Collecting scipy==1.4.1\n",
      "  Using cached scipy-1.4.1-cp38-cp38-win_amd64.whl (31.0 MB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in c:\\users\\nguye\\anaconda3\\lib\\site-packages (from tensorflow==2.2) (1.19.5)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in c:\\users\\nguye\\anaconda3\\lib\\site-packages (from tensorflow==2.2) (1.1.2)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in c:\\users\\nguye\\anaconda3\\lib\\site-packages (from tensorflow==2.2) (3.17.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\nguye\\anaconda3\\lib\\site-packages (from tensorflow==2.2) (3.3.0)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\nguye\\anaconda3\\lib\\site-packages (from tensorflow==2.2) (0.36.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\nguye\\anaconda3\\lib\\site-packages (from tensorflow==2.2) (1.15.0)\n",
      "Collecting gast==0.3.3\n",
      "  Using cached gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
      "  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
      "Requirement already satisfied: astunparse==1.6.3 in c:\\users\\nguye\\anaconda3\\lib\\site-packages (from tensorflow==2.2) (1.6.3)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in c:\\users\\nguye\\anaconda3\\lib\\site-packages (from tensorflow==2.2) (1.38.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in c:\\users\\nguye\\anaconda3\\lib\\site-packages (from tensorflow==2.2) (0.2.0)\n",
      "Collecting h5py<2.11.0,>=2.10.0\n",
      "  Using cached h5py-2.10.0-cp38-cp38-win_amd64.whl (2.5 MB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\nguye\\anaconda3\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (2.25.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\nguye\\anaconda3\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (52.0.0.post20210125)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\nguye\\anaconda3\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.0.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\nguye\\anaconda3\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.21.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\nguye\\anaconda3\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (0.4.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\nguye\\anaconda3\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\nguye\\anaconda3\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.8.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\nguye\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (0.2.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\nguye\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\nguye\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\nguye\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\nguye\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nguye\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\nguye\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\nguye\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\nguye\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.26.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\nguye\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.1.1)\n",
      "Installing collected packages: tensorflow-estimator, tensorboard, scipy, h5py, gast, tensorflow\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.5.0\n",
      "    Uninstalling tensorflow-estimator-2.5.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.5.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.5.0\n",
      "    Uninstalling tensorboard-2.5.0:\n",
      "      Successfully uninstalled tensorboard-2.5.0\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.6.2\n",
      "    Uninstalling scipy-1.6.2:\n",
      "      Successfully uninstalled scipy-1.6.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\nguye\\\\anaconda3\\\\Lib\\\\site-packages\\\\~cipy\\\\linalg\\\\cython_blas.cp38-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfec665b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LayerNormalization' from 'tensorflow.python.keras.layers.normalization' (C:\\Users\\nguye\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\normalization\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-67e151f94a7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# from tensorflow.keras import layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_column\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfeature_column_lib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfeature_column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# See b/110718070#comment18 for more details about this import.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\models.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtraining_v1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayers\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbase_layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;31m# Normalization layers.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalization\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLayerNormalization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalization_v2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSyncBatchNormalization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'LayerNormalization' from 'tensorflow.python.keras.layers.normalization' (C:\\Users\\nguye\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\normalization\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "# from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6871fc60",
   "metadata": {},
   "source": [
    "## Train, Validation, Test Set Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a3bd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ./dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e485ced4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil # Provides high-level operations on files and collections of files\n",
    "import os\n",
    "\n",
    "# Define original image path, training (features + labels), and testing (features)\n",
    "src_dir = \"./dataset/images\"\n",
    "train_val_dir = \"./dataset/working/train_val_images\"\n",
    "test_dir = \"./dataset/working/test_images\"\n",
    "\n",
    "# Create folders\n",
    "if not os.path.isdir(train_val_dir):\n",
    "    os.mkdir(train_val_dir)\n",
    "if not os.path.isdir(test_dir):\n",
    "    os.mkdir(test_dir)\n",
    "\n",
    "# Put images in correct folder\n",
    "if len([f for f in os.listdir(test_dir)]) == 0:\n",
    "    all_images_names = os.listdir(src_dir)\n",
    "    train_val_images = []\n",
    "    test_images = []\n",
    "    for image in all_images_names:\n",
    "        if \"Train\" in image:\n",
    "            shutil.copy(src_dir+\"/\"+image, train_val_dir)\n",
    "        elif \"Test\" in image:\n",
    "            shutil.copy(src_dir+\"/\"+image, test_dir)\n",
    "        else:\n",
    "            print(\"Can't place image in Train or Test folder\")\n",
    "\n",
    "# Check for errors\n",
    "total = len([file for file in os.listdir(src_dir)])\n",
    "train_val_total = len([file for file in os.listdir(train_val_dir)])\n",
    "test_total = len([file for file in os.listdir(test_dir)])\n",
    "print(f\"It is {total == train_val_total + test_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d43a88e",
   "metadata": {},
   "source": [
    "## Import module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064681fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tf-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bb3b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176108fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e28f79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a94d3f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf8fb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839d742d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command for jupyter notebook interaction\n",
    "%pylab inline \n",
    "\n",
    "# Basic utilities\n",
    "import sys # Enabler of operating system dependent functionality\n",
    "import os  # Provides access to some variables & functions for the interpreter\n",
    "from shutil import copyfile # Import module we'll need to import our custom module\n",
    "import math # Provides access to basic mathematical functions\n",
    "import time # Provides various time-related functions\n",
    "import glob # Pathnames management\n",
    "from PIL import Image as pil_image\n",
    "import itertools\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import expand_dims\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib # Interface for creation of publication-quality plots and figures\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns # Matplotlib-based statistical data visualization interface \n",
    "\n",
    "# Plotly\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Scikit-Learn\n",
    "from sklearn.model_selection import train_test_split # split arrays or matrices into random train and test subsets\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.decomposition import PCA, TruncatedSVD # Principal component analysis (PCA), dimensionality reduction using truncated SVD\n",
    "\n",
    "# Tensorflow - Keras - TF version 2.5\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img, save_img\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow.keras.layers as L\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "\n",
    "# SMOTE\n",
    "from imblearn.over_sampling import SMOTE # Class Balancing tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208d7ca4",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfd12d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = r\"dataset/working/\"\n",
    "indir = r\"dataset/\"\n",
    "\n",
    "test_dir = indir + \"working/test_images\"\n",
    "train_labels_csv = pd.read_csv(indir+\"train.csv\")\n",
    "print(\"Train Labels\")\n",
    "print(train_labels_csv.head())\n",
    "print(\"--------------------------------------------------\\n\")\n",
    "\n",
    "example_submission_csv = pd.read_csv(indir+\"sample_submission.csv\")\n",
    "print(\"Sample Submission\")\n",
    "print(example_submission_csv.head())\n",
    "print(\"--------------------------------------------------\\n\")\n",
    "\n",
    "print(\"Test Images\")\n",
    "test_csv = pd.read_csv(indir+\"test.csv\")\n",
    "test_paths_csv = pd.DataFrame(test_csv[\"image_id\"].apply(lambda x: test_dir+\"/\"+x+\".jpg\"))\n",
    "print(test_paths_csv.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a849899",
   "metadata": {},
   "source": [
    "## Train, Validation Spliting - Data Preprocessing\n",
    "- Creates 4 directories: healthy, multiple_diseases, rust, scab with the corresponding images from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0d0d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create csv files with images\n",
    "train_val_healthy_csv = train_labels_csv[train_labels_csv[\"healthy\"] == 1]\n",
    "train_val_multiple_diseases_csv  = train_labels_csv[train_labels_csv[\"multiple_diseases\"] == 1]\n",
    "train_val_rust_csv = train_labels_csv[train_labels_csv[\"rust\"] == 1]\n",
    "train_val_scab_csv = train_labels_csv[train_labels_csv[\"scab\"] == 1]\n",
    "\n",
    "train_val_healthy_names = train_val_healthy_csv[\"image_id\"].tolist()\n",
    "train_val_multiple_diseases_names = train_val_multiple_diseases_csv[\"image_id\"].tolist()\n",
    "train_val_rust_names = train_val_rust_csv[\"image_id\"].tolist()\n",
    "train_val_scab_names = train_val_scab_csv[\"image_id\"].tolist()\n",
    "\n",
    "src_dir = indir + \"working/train_val_images\"\n",
    "train_dst_dir = outdir + \"train_im\"\n",
    "\n",
    "train_dst_healthy_dir = outdir + \"train_im/healthy\"\n",
    "train_dst_multiple_diseases_dir = outdir + \"train_im/multiple_diseases\"\n",
    "train_dst_rust_dir = outdir + \"train_im/rust\"\n",
    "train_dst_scab_dir = outdir + \"train_im/scab\"\n",
    "\n",
    "test_dst_dir = outdir + \"test_im/test\"\n",
    "\n",
    "# Create and fill the directories\n",
    "try:\n",
    "    os.mkdir(test_dst_dir)\n",
    "    os.mkdir(train_dst_dir)\n",
    "    os.mkdir(train_dst_healthy_dir)\n",
    "    os.mkdir(train_dst_multiple_diseases_dir)\n",
    "    os.mkdir(train_dst_rust_dir)\n",
    "    os.mkdir(train_dst_scab_dir)\n",
    "    \n",
    "    for image in train_val_healthy_names:\n",
    "        shutil.copy(src_dir+\"/\"+image+\".jpg\", train_dst_healthy_dir)\n",
    "    \n",
    "    for image in train_val_multiple_diseases_names:\n",
    "        shutil.copy(src_dir+\"/\"+image+\".jpg\", train_dst_multiple_diseases_dir)\n",
    "    \n",
    "    for image in train_val_rust_names:\n",
    "        shutil.copy(src_dir+\"/\"+image+\".jpg\", train_dst_rust_dir)\n",
    "    \n",
    "    for image in train_val_scab_names:\n",
    "        shutil.copy(src_dir+\"/\"+image+\".jpg\", train_dst_scab_dir)\n",
    "        \n",
    "    for image in test_paths_csv[\"image_id\"].tolist():\n",
    "        shutil.copy(image, test_dst_dir)\n",
    "\n",
    "except FileExistsError as err:\n",
    "    print(\"Folders already exist.\")\n",
    "    \n",
    "# Check for errors\n",
    "total = len([file for file in os.listdir(src_dir)])\n",
    "train_healthy_total = len([file for file in os.listdir(train_dst_healthy_dir)])\n",
    "train_multiple_diseases_total = len([file for file in os.listdir(train_dst_multiple_diseases_dir)])\n",
    "train_rust_total = len([file for file in os.listdir(train_dst_rust_dir)])\n",
    "train_scab_total = len([file for file in os.listdir(train_dst_scab_dir)])\n",
    "\n",
    "total = train_healthy_total + train_multiple_diseases_total + train_rust_total + train_scab_total\n",
    "train_size = math.ceil(total*0.8)\n",
    "val_size = total - train_size\n",
    "test_size = test_csv.size\n",
    "image_size = (200,200)\n",
    "batch_size = 32\n",
    "seed = 100\n",
    "print(train_healthy_total, train_multiple_diseases_total, train_rust_total, train_scab_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5789434f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee42381",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd09fefa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed7a075",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0a42b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4738fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523dde4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc54a41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e63a68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
